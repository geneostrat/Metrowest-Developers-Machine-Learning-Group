{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "The sklearn.feature_extraction module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector output: \n",
      "[[  41.    0.    0.    0.    1.    0.    0.]\n",
      " [   8.    0.    0.    0.    0.    1.    0.]\n",
      " [  16.    0.    1.    0.    0.    0.    0.]\n",
      " [  11.    0.    0.    0.    0.    0.    1.]\n",
      " [  43.    0.    0.    1.    0.    0.    0.]\n",
      " [ 122.    1.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "feature names:  ['sales', 'territory=Asia', 'territory=Central', 'territory=Europe', 'territory=Northeast', 'territory=SouthEast', 'territory=West']\n"
     ]
    }
   ],
   "source": [
    "# SKL-FE1\n",
    "# demonstrate one-hot (one-of-K) encode of territory names\n",
    "\n",
    "region_sales = [\n",
    "    {'territory': 'Northeast', 'sales': 41},       # define unique sales territories\n",
    "    {'territory': 'SouthEast', 'sales': 8},\n",
    "    {'territory': 'Central', 'sales': 16},\n",
    "    {'territory': 'West', 'sales': 11},\n",
    "    {'territory': 'Europe', 'sales': 43},\n",
    "    {'territory': 'Asia', 'sales': 122},           # Python permits trailing comma\n",
    "]\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer    # import namespace\n",
    "vec = DictVectorizer()                                   # create feature extraction instance\n",
    "print('vector output: ')\n",
    "print(vec.fit_transform(region_sales).toarray())         # transform and format for printing\n",
    "print()\n",
    "print('feature names: ', vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
       "       [ 1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SKL-FE2\n",
    "# feature hasher\n",
    "# high-speed, low-memory vectorizer (does not support an inverse transform)\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "hash = FeatureHasher(n_features=10, input_type='string')\n",
    "data = [['Asia', 'Europe', 'Asia'],     # Note: 'Asia' appears twice (see transform output)\n",
    "        ['Asia', 'Northeast']]\n",
    "output = hash.transform(data)\n",
    "output.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SKL-FE3\n",
    "# CountVectorizer defaults\n",
    "# CountVectorizer implements both tokenization and occurrence counting in a single class\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer transform: \n",
      "[[0 3 2 3 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 1 0 0 1 0 0 2 1 0 1]\n",
      " [0 3 2 3 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 1 0 1 1 0 0 1 0]]\n",
      "\n",
      "['all', 'da', 'de', 'do', 'innocence', 'is', 'me', 'pull', 'say', 'their', 'through', 'to', 'want', 'will', 'you']\n",
      "\n",
      "get \"will\" feature index:  13\n"
     ]
    }
   ],
   "source": [
    "# SKL-FE4\n",
    "# train a countvectorizer\n",
    "# dump vector map\n",
    "# print feature names\n",
    "# locate a feature index\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "documents = [\n",
    "    'De do do do, de da da da',\n",
    "    'Is all I want to say to you',\n",
    "    'De do do do, de da da da',\n",
    "    'Their innocence will pull me through',\n",
    "]\n",
    "X = vectorizer.fit_transform(documents)\n",
    "print('vectorizer transform: ')\n",
    "print(X.toarray())\n",
    "print()\n",
    "print(vectorizer.get_feature_names())\n",
    "print()\n",
    "print('get \"will\" feature index: ',vectorizer.vocabulary_.get('will'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SKL-FE5\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "documents = [\n",
    "    'De do do do, de da da da',\n",
    "    'Is all I want to say to you',\n",
    "    'De do do do, de da da da',\n",
    "    'Their innocence will pull me through',\n",
    "]\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# run a new phrase through the trained data (Note: 'to' and 'their' are identified, as they appear in the training)\n",
    "vectorizer.transform([\"Poets priests and politicians, Have words to thank for their positions\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized array:  [[0 0 3 2 2 1 1 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 2 1 1 1 1 0 0 1]\n",
      " [0 0 3 2 2 1 1 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0]]\n",
      "Index of \"do\":  7\n",
      "Index of \"de do\":  6\n",
      "Index of \"da da\":  3\n",
      "Index of \"innocence will\":  13\n",
      "Index of \"will pull\":  31\n"
     ]
    }
   ],
   "source": [
    "# SKL-FE6\n",
    "# 2-gram and 1-gram word extractions \n",
    "# vectorized output is larger \n",
    "# preserves some of the local ordering information \n",
    "  \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "documents = [ \n",
    "    'De do do do, de da da da', \n",
    "    'Is all I want to say to you', \n",
    "    'De do do do, de da da da', \n",
    "    'Their innocence will pull me through', \n",
    "] \n",
    "  \n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),                  # extract 1 and 2 word combinations \n",
    "                                    token_pattern=r'\\b\\w+\\b', min_df=1)  # \\b matches 'zero-width' before the character \n",
    "X_2 = bigram_vectorizer.fit_transform(documents).toarray()               # \\w 'word' characters [A-Za-z0-9_]  \n",
    "print('vectorized array: ',X_2) \n",
    "  \n",
    "print('Index of \"do\": ',bigram_vectorizer.vocabulary_.get('do')) \n",
    "print('Index of \"de do\": ',bigram_vectorizer.vocabulary_.get('de do')) \n",
    "print('Index of \"da da\": ',bigram_vectorizer.vocabulary_.get('da da')) \n",
    "print('Index of \"innocence will\": ',bigram_vectorizer.vocabulary_.get('innocence will'))   # word prior to 'will' \n",
    "print('Index of \"will pull\": ',bigram_vectorizer.vocabulary_.get('will pull'))             # word after 'will' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
